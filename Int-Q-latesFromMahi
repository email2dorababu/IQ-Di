19/May/2016

1. How to transfer data from two data centers
2. what you do when data transfer is failed while copying the data


Hive:
how to run 100 commands at a time in hive?
when 80 are succesful and 20 are failed, where to check what commands are failed?

Pig:
Difference between group and cogroup

MR
how to run all small files as one MR program? 
Where is counters information is stored?

How to check a file, on what datanodes it is stored?
---------------------------------------------------
Date 19May2016
1) -Check the block locations of each block
hadoop fsck filename/foldername -files -blocks -locations
hadoop is dpricated, use hdfs fsck.
https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#fsck

2) Precautions you take while copying data with DISCP
https://hadoop.apache.org/docs/r2.7.2/hadoop-distcp/DistCp.html
use -update to update the fiels with the latest version

3)How to run 100 queries parallely ? 
https://www.qubole.com/blog/big-data/5-tips-for-efficient-hive-queries/?nabe=5695374637924352:0&utm_referrer=https%3A%2F%2Fwww.google.co.in%2F
SET hive.exce.parallel=true;   several queries executed on Hive make automatically use of MR parallel execution
Set this property in hive-site.xml

4) can hive script should be of .hql extension or any extension?
you can use any extension to run hive scripts. for example
hive -f scriptname.sh
but using .hql is is the best practice so that you can differentiate this file from other files.
The standard at Apache Hive project is to use .q

5) How to set Reducers in MR(Yarn), pig and Hive?
https://hadoopjournal.wordpress.com/2015/05/30/set-reducers-in-pig-hive-and-mapreduce/
MR (yarn)
-------
-D mapreduce.job.reduces=XX
conf.set("mapreduce.job.reduces", "4");  -- using configuration
job.setNumReduceTasks(10); -- using Job
Old version - mapred.reduce.tasks

Pig: set no of reducers
-----
SET default_parallel XXX
In a script, the value we set using the PARALLEL clause will override any value we specify through “SET default_parallel.”

A = LOAD 'myfile' AS (t, u, v);
B = GROUP A BY t PARALLEL 18;   --  Here 18 is number of reducer
pig.exec.reducers.bytes.per.reducer – Defines the number of input bytes per reduce; default value is 1000*1000*1000 (1GB).

Hive
-----
Set the number of reducer in HIVE:
SET mapred.reduce.tasks=XX // In Hadoop 1.X
SET mapreduce.job.reduces=XX // In Hadoop 2.X (YARN)

Set this property before your hive query.

Example:
SET mapreduce.job.reduces=10;select count(*) from table1;

Default is -1, with this, hive will choose the reducers based on the data other property, hive.exec.reducers.bytes.per.reducer
Default Value: 1000000000, 1G, i.e 1G data one reducer



To control the max reducers, (when the value is mapreduce.job.reduces -1)
hive.exec.reducers.max
Default Value: 999

6) How to find failed queries? 
----------------------------
see - http://stackoverflow.com/questions/35902860/track-success-or-failure-for-hive-queries-running-through-shell-script
The exit code rom the previous command is stored in the variable $?

#!/bin/bash  
HQLSource='/home/hql/'  
if hive -f "$HQLSource/query1.hql"
then
    echo Query 1 succeeded
else
    echo Query 1 failed
fi

7) what is vectorized exectuion in Hive/
https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution
http://hortonworks.com/blog/5-ways-make-hive-queries-run-faster/
https://www.qubole.com/blog/big-data/hive-best-practices/?nabe=5695374637924352:0&utm_referrer=https%3A%2F%2Fwww.google.co.in%2F

8) list out all the job
hadoop job -list all

use mapred instead of hadoop.
9) how to get specif counter value??

hadoop job -counter jobid groupname countername
https://books.google.co.in/books?id=Wu_xeGdU4G8C&pg=PA167&lpg=PA167&dq=what+is+the+location+of+job+output+directory+in+hadoop&source=bl&ots=i8zPVCP7Ys&sig=MoQ7M-7jdbP6Dkc2J7lfKrAllL4&hl=en&sa=X&ved=0ahUKEwiJ_amukubMAhUlw4MKHayJDWgQ6AEIRDAG#v=onepage&q=what%20is%20the%20location%20of%20job%20output%20directory%20in%20hadoop&f=false

10) Tuning Mapreduce - 
See Tuning Checklist in hadoop definitive guide.

11) Hadoop Command reference Very Good 
http://www.thegeekstuff.com/2015/02/hadoop-command-reference/

12) to get the YARN applications running
yarn application -list
yarn application -kill appid
yarn application -movetoqueue appid -queue queuename
yarn application -status appid

13) load data with uneven spaces in between them into hive/pig.

https://martin.atlassian.net/wiki/pages/viewpage.action?pageId=21299205
http://stackoverflow.com/questions/14338495/import-data-into-hive-containing-whitespace

14) Compresson formats used in your project, and what compression techniques are used ?


